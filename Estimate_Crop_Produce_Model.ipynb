{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Model to Predict Crop Production in Kharif Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('df_crop_produce.pickle', 'rb') as handle:\n",
    "    df_crop_produce = pickle.load(handle)\n",
    "\n",
    "with open('df_rainfall.pickle', 'rb') as handle:\n",
    "    df_rainfall = pickle.load(handle)\n",
    "    \n",
    "with open('df_temp.pickle', 'rb') as handle:\n",
    "    df_temp = pickle.load(handle)\n",
    "    \n",
    "#Compute the Rainfall in centimeters during Kharif season (July-Oct)\n",
    "rainfall_kharif = df_rainfall[df_rainfall['Parameter'] == 'Actual'].groupby(['YEAR'])['Khariff'].sum()/100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few records of Rainfall\n",
      "                   SUBDIVISION  YEAR             Parameter   JAN    FEB  \\\n",
      "276  ANDAMAN & NICOBAR ISLANDS  1997                Actual   9.5    0.0   \n",
      "277  ANDAMAN & NICOBAR ISLANDS  1997  Percentage departure -80.8 -100.0   \n",
      "278  ANDAMAN & NICOBAR ISLANDS  1997      No. of districts   3.0    3.0   \n",
      "279  ANDAMAN & NICOBAR ISLANDS  1998                Actual   0.9    0.0   \n",
      "280  ANDAMAN & NICOBAR ISLANDS  1998  Percentage departure -98.2 -100.0   \n",
      "\n",
      "       MAR    APR    MAY    JUN    JUL  ...      OCT    NOV   DEC  ANNUAL  \\\n",
      "276    0.2   15.6  281.1  199.5  918.5  ...    128.7  292.8  38.4  2755.1   \n",
      "277  -99.3  -78.4  -21.0  -57.7  131.1  ...    -55.5   25.7 -75.0    -5.4   \n",
      "278    3.0    2.0    3.0    3.0    1.0  ...      1.0    1.0   1.0     NaN   \n",
      "279    0.0    0.0  348.9  600.0  364.5  ...    618.6  227.8  89.0  2846.4   \n",
      "280 -100.0 -100.0   -1.9   27.3   -8.3  ...    113.7   -2.2 -42.0    -2.2   \n",
      "\n",
      "       JF    MAM    JJAS    OND  Khariff   Rabi  \n",
      "276   9.5  296.9  1988.8  459.9   1918.0  469.6  \n",
      "277 -87.7  -35.1    16.9  -31.9     85.2 -384.9  \n",
      "278   NaN    NaN     NaN    NaN      4.0   12.0  \n",
      "279   0.9  348.9  1561.2  935.4   1579.8  936.3  \n",
      "280 -98.8  -23.8    -8.2   38.4     48.3 -228.7  \n",
      "\n",
      "[5 rows x 22 columns] \n",
      "\n",
      "First few records of Temperature\n",
      "     YEAR    JAN    FEB    MAR    APR    MAY    JUN    JUL    AUG    SEP  \\\n",
      "96   1997  17.86  19.88  23.64  25.55  27.86  28.33  28.01  27.27  26.81   \n",
      "97   1998  18.84  20.60  22.98  27.00  29.18  28.88  27.78  27.42  26.70   \n",
      "98   1999  18.32  21.26  24.18  27.66  28.13  27.95  27.58  27.08  26.90   \n",
      "99   2000  18.87  19.78  23.22  27.27  28.92  28.02  27.34  26.98  26.53   \n",
      "100  2001  18.50  21.00  24.12  26.90  29.46  28.13  27.63  27.62  26.86   \n",
      "\n",
      "       OCT    NOV    DEC  ANNUAL  JAN-FEB  MAR-MAY  JUN-SEP  OCT-DEC  Kharif  \\\n",
      "96   24.48  22.05  19.31   24.10    18.87    25.69    27.60    21.89   26.64   \n",
      "97   25.27  22.48  19.21   24.76    19.72    26.41    27.69    22.42   26.79   \n",
      "98   24.97  22.33  19.57   24.67    19.75    26.66    27.38    22.29   26.63   \n",
      "99   25.58  22.75  19.66   24.60    19.33    26.47    27.23    22.68   26.61   \n",
      "100  25.49  22.51  19.55   24.73    19.75    26.82    27.47    22.52   26.90   \n",
      "\n",
      "      Rabi  \n",
      "96   21.20  \n",
      "97   21.56  \n",
      "98   21.77  \n",
      "99   21.64  \n",
      "100  21.86   \n",
      "\n",
      "First few records of Crop produce data\n",
      "                    State_Name District_Name  Crop_Year      Season  \\\n",
      "0  Andaman and Nicobar Islands      NICOBARS       2000      Kharif   \n",
      "1  Andaman and Nicobar Islands      NICOBARS       2000      Kharif   \n",
      "2  Andaman and Nicobar Islands      NICOBARS       2000      Kharif   \n",
      "3  Andaman and Nicobar Islands      NICOBARS       2000  Whole Year   \n",
      "4  Andaman and Nicobar Islands      NICOBARS       2000  Whole Year   \n",
      "\n",
      "                  Crop    Area  Production  \n",
      "0             Arecanut  1254.0      2000.0  \n",
      "1  Other Kharif pulses     2.0         1.0  \n",
      "2                 Rice   102.0       321.0  \n",
      "3               Banana   176.0       641.0  \n",
      "4            Cashewnut   720.0       165.0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('First few records of Rainfall')\n",
    "print(df_rainfall.head(), '\\n')\n",
    "\n",
    "print('First few records of Temperature')\n",
    "print(df_temp.head(), '\\n')\n",
    "\n",
    "print('First few records of Crop produce data')\n",
    "print(df_crop_produce.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset that contains the crop production data, Temperature data and Rainfall data pertaining to Khariff season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Production_Count  Area_Count     Area_Sum  Kharif  Khariff  Production_Sum\n",
      "0               3415        3415  75682249.00   26.64  329.360    1.382803e+08\n",
      "1               4563        4708  83779058.00   26.79  368.052    1.610406e+08\n",
      "2               5351        5373  78214722.00   26.63  334.555    1.107851e+08\n",
      "3               5981        6025  86812995.00   26.61  289.456    1.142366e+08\n",
      "4               5846        5873  87807709.40   26.90  289.739    1.277101e+08\n",
      "5               5805        5865  80031223.00   26.90  250.052    9.665179e+07\n",
      "6               6145        6161  88706585.43   26.61  331.896    1.382855e+08\n",
      "7               5412        5552  80700959.42   26.44  298.543    2.399453e+08\n",
      "8               5335        5510  82910545.47   26.60  352.631    2.910593e+08\n",
      "9               5148        5303  82028612.17   26.72  336.148    2.615515e+08\n",
      "10              5347        5472  70839304.00   26.64  367.336    2.525192e+08\n",
      "11              5350        5479  82601771.00   26.60  311.016    3.030534e+08\n",
      "12              4757        4906  72563659.00   27.00  294.109    2.952903e+08\n",
      "13              5589        5723  89264102.81   26.76  354.293    3.653053e+08\n",
      "14              5223        5366  75118880.56   26.75  332.394    2.781617e+08\n",
      "15              5433        5514  69277092.00   26.70  325.184    2.859547e+08\n",
      "16              5221        5286  66443853.00   26.80  345.948    2.992172e+08\n",
      "17              4339        4397  52000480.50   26.87  316.974    2.708229e+08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "#Compute the total number of crops produced in Kharif season and total number of districts producing crops in Kharif Season.\n",
    "df_crop_count = df_crop_produce[ (df_crop_produce['Season'] == 'Kharif') ]\\\n",
    "          .groupby(['Crop_Year'])[['Production', 'Area']].count()\n",
    "df_crop_count.rename(columns={'Production':'Production_Count', 'Area':'Area_Count'}, inplace=True) \n",
    "\n",
    "#Compute the total Yeild of crops produced in Kharif season and total land Area used for cultivation in Kharif Season.\n",
    "df_crop_sum = df_crop_produce[ (df_crop_produce['Season'] == 'Kharif') ]\\\n",
    "          .groupby(['Crop_Year'])[['Production', 'Area']].sum()\n",
    "df_crop_sum.rename(columns={'Production':'Production_Sum', 'Area':'Area_Sum'}, inplace=True)\n",
    "\n",
    "df_crop_count.reset_index(drop=True, inplace=True)\n",
    "df_crop_sum.reset_index(drop=True, inplace=True) \n",
    "rainfall_kharif.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Concatenate all rainfall data, temperature data and crop production data\n",
    "df = pd.concat([df_crop_count, df_crop_sum, df_temp['Kharif'].reset_index(drop=True) , \\\n",
    "                rainfall_kharif ], axis = 1)\n",
    "\n",
    "#remove the 2015 record as the crop production data in that year is incomplete\n",
    "df = df[:-1]\n",
    "\n",
    "#rearranging the columns of the dataframe\n",
    "df = df[['Production_Count', 'Area_Count', 'Area_Sum', 'Kharif', 'Khariff', 'Production_Sum']]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records are too less. \n",
    "\n",
    "In the first attempt will build a MultiLayer Perceptron Regressor model with 14 records for training and 3 records for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivanand/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:85: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  Loss:  231.38388\n",
      "960  Loss:  211.64696\n",
      "920  Loss:  196.4836\n",
      "880  Loss:  182.84583\n",
      "840  Loss:  170.69438\n",
      "800  Loss:  160.00078\n",
      "760  Loss:  150.73093\n",
      "720  Loss:  142.8371\n",
      "680  Loss:  136.25078\n",
      "640  Loss:  130.88953\n",
      "600  Loss:  126.65739\n",
      "560  Loss:  123.39828\n",
      "520  Loss:  120.90074\n",
      "480  Loss:  118.97827\n",
      "440  Loss:  117.51416\n",
      "400  Loss:  116.4211\n",
      "360  Loss:  115.6083\n",
      "320  Loss:  114.997154\n",
      "280  Loss:  114.524345\n",
      "240  Loss:  114.14736\n",
      "200  Loss:  113.83958\n",
      "160  Loss:  113.58618\n",
      "120  Loss:  113.37526\n",
      "80  Loss:  113.19923\n",
      "40  Loss:  113.05195\n",
      "\n",
      "\n",
      "Predicted vs Actual Values \n",
      "\n",
      "240676136.24 299217189.78\n",
      "244250087.44 270822856.5\n",
      "206162425.54 261551483.26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import pickle, os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def trainModel(X_train, X_test, y_train, y_test, scaler_Clf, is_feedback=False):     \n",
    "\n",
    "    with tf.Graph().as_default():        \n",
    "        N, D = X_train.shape\n",
    "        \n",
    "        #Build 3 Hidden Layers with the following sizes\n",
    "        M = 30\n",
    "        H1 = 45\n",
    "        H2 = 30\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, D))\n",
    "        y = tf.placeholder(tf.float32, shape=(None))\n",
    "\n",
    "        # input -> hidden1\n",
    "        W = tf.Variable(tf.random_normal(shape=(D, M)) * np.sqrt(2.0 / M))\n",
    "        b = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "\n",
    "        # hidden1 -> hidden2\n",
    "        W1 = tf.Variable(tf.random_normal(shape=(M, H1)) * np.sqrt(2.0 / H1))\n",
    "        b1 = tf.Variable(np.zeros(H1).astype(np.float32))\n",
    "        \n",
    "        # hidden2 -> hidden3\n",
    "        W2 = tf.Variable(tf.random_normal(shape=(H1, H2)) * np.sqrt(2.0 / H2))\n",
    "        b2 = tf.Variable(np.zeros(H2).astype(np.float32))\n",
    "\n",
    "        # hidden3 -> output\n",
    "        V = tf.Variable(tf.random_normal(shape=(H2, 1)) * np.sqrt(2.0 / D))\n",
    "        c = tf.Variable(np.zeros(1).astype(np.float32))\n",
    "\n",
    "        # construct the reconstruction\n",
    "        Z = tf.nn.tanh(tf.matmul(X, W) + b)\n",
    "        Zh1 = tf.nn.tanh(tf.matmul(Z, W1) + b1) \n",
    "        Zh2 = tf.nn.tanh(tf.matmul(Zh1, W2) + b2) \n",
    "        logits = tf.matmul(Zh2, V) + c\n",
    "        X_hat = (logits)\n",
    "\n",
    "        cost = tf.losses.mean_squared_error(logits, y) + tf.nn.l2_loss(V) \n",
    "        \n",
    "        # make the trainer, using RMS optimiser\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=0.00001).minimize(cost)\n",
    "\n",
    "        # set up session and variables for later\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        n_epochs = 1000  \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        #When deploying in GPU enabled machines\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.1 \n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init_op)\n",
    "            if os.path.isfile(\"./final_model.ckpt\"):\n",
    "                saver.restore(sess, \"./final_model.ckpt\")\n",
    "            while n_epochs > 0:\n",
    "                sess.run(train_op, feed_dict={X:X_train, y:y_train})\n",
    "                if n_epochs%40 == 0:\n",
    "                    print(n_epochs, ' Loss: ', sess.run(cost, feed_dict={X:X_train, y:y_train}))\n",
    "                n_epochs = n_epochs - 1\n",
    "            res = sess.run(X_hat, feed_dict={X:X_test}) \n",
    "            saver.save(sess, \"./final_model.ckpt\")\n",
    "        \n",
    "        print('\\n')\n",
    "        print('Predicted vs Actual Values', '\\n')\n",
    "        test_records = []\n",
    "        for i, test in enumerate(X_test):\n",
    "            pred = round(clf.inverse_transform(np.append(test, res[i]))[-1],2)\n",
    "            real = round(clf.inverse_transform(np.append(test, y_test[i]))[-1],2)\n",
    "            print(pred, real)\n",
    "            test_records.append(clf.inverse_transform(np.append(test, y_test[i])))\n",
    "            \n",
    "        if not is_feedback:\n",
    "            with open('test_records.pickle', 'wb') as handle:\n",
    "                pickle.dump(test_records, handle, protocol=pickle.HIGHEST_PROTOCOL)                        \n",
    "                                                      \n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(39)\n",
    "    np_df = df.as_matrix()\n",
    "    np.random.shuffle(np_df)\n",
    " \n",
    "    clf = StandardScaler() \n",
    "    #Fit only the training data\n",
    "    clf.fit(np_df[:15,:])\n",
    "    np_df = clf.transform(np_df)\n",
    "    X_Hi = np_df[:,:-1]\n",
    "    y_Hi = np_df[:, -1]\n",
    "    \n",
    "    X_train, X_test = X_Hi[:15, :], X_Hi[15:, :]\n",
    "    y_train, y_test = y_Hi[:15], y_Hi[15:]\n",
    "    trainModel(X_train, X_test, y_train, y_test, clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the shortage of data points the model performed fairly decently.\n",
    "\n",
    "In the next attempt, would try to generate additional data points using Autoencoders and then rebuild a Multi Layer perceptron model using this augmented dataset for training. Idea is to see if there is any improvement in prediction of the test set between the 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivanand/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:79: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batches: 1\n",
      "epoch: 0\n",
      "epoch: 10\n",
      "epoch: 20\n",
      "epoch: 30\n",
      "epoch: 40\n",
      "epoch: 50\n",
      "epoch: 60\n",
      "epoch: 70\n",
      "epoch: 80\n",
      "epoch: 90\n",
      "epoch: 100\n",
      "epoch: 110\n",
      "epoch: 120\n",
      "epoch: 130\n",
      "epoch: 140\n",
      "epoch: 150\n",
      "epoch: 160\n",
      "epoch: 170\n",
      "epoch: 180\n",
      "epoch: 190\n",
      "epoch: 200\n",
      "epoch: 210\n",
      "epoch: 220\n",
      "epoch: 230\n",
      "epoch: 240\n",
      "epoch: 250\n",
      "epoch: 260\n",
      "epoch: 270\n",
      "epoch: 280\n",
      "epoch: 290\n",
      "epoch: 300\n",
      "epoch: 310\n",
      "epoch: 320\n",
      "epoch: 330\n",
      "epoch: 340\n",
      "epoch: 350\n",
      "epoch: 360\n",
      "epoch: 370\n",
      "epoch: 380\n",
      "epoch: 390\n",
      "Mean cost in the last 10 epochs:  0.0008323811268543497\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/deep-learning-gans-and-variational-autoencoders\n",
    "# https://www.udemy.com/deep-learning-gans-and-variational-autoencoders\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, D, M, H1, H2, H3):\n",
    "        # represents a batch of training data\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, D))\n",
    "\n",
    "        # input -> hidden1\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(D, M)) * np.sqrt(2.0 / (D+M)))\n",
    "        self.b = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "\n",
    "        # hidden1 -> hidden2\n",
    "        self.W1 = tf.Variable(tf.random_normal(shape=(M, H1)) * np.sqrt(2.0 / (M+H1)))\n",
    "        self.b1 = tf.Variable(np.zeros(H1).astype(np.float32))\n",
    "\n",
    "        # hidden1 -> hidden2\n",
    "        self.W2 = tf.Variable(tf.random_normal(shape=(H1, H2)) * np.sqrt(2.0 / (H1+H2)))\n",
    "        self.b2 = tf.Variable(np.zeros(H2).astype(np.float32))\n",
    "        \n",
    "        # hidden1 -> hidden2\n",
    "        self.W3 = tf.Variable(tf.random_normal(shape=(H2, H3)) * np.sqrt(2.0 / (H2+H3)))\n",
    "        self.b3 = tf.Variable(np.zeros(H3).astype(np.float32))\n",
    "        \n",
    "        # hidden2 -> output\n",
    "        self.V = tf.Variable(tf.random_normal(shape=(H3, D)) )\n",
    "        self.c = tf.Variable(np.zeros(D).astype(np.float32))\n",
    "\n",
    "        # construct the reconstruction\n",
    "        self.Z = tf.nn.relu(tf.matmul(self.X, self.W) + self.b)\n",
    "        self.Zh = tf.nn.relu(tf.matmul(self.Z, self.W1) + self.b1)\n",
    "        self.Zh1 = tf.nn.relu(tf.matmul(self.Zh, self.W2) + self.b2)\n",
    "        self.Zh2 = tf.nn.relu(tf.matmul(self.Zh1, self.W3) + self.b3)\n",
    "        logits = tf.matmul(self.Zh2, self.V) + self.c\n",
    "        self.X_hat = (logits)\n",
    "\n",
    "    \n",
    "        self.cost = tf.nn.l2_loss(logits - self.X) \n",
    "        # make the trainer\n",
    "        self.train_op = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(self.cost)\n",
    "\n",
    "        # set up session and variables for later\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(self.init_op)\n",
    "\n",
    "    def fit(self, X, epochs=400, batch_sz=17):\n",
    "        costs = []\n",
    "        n_batches = len(X) // batch_sz \n",
    "        tf.reset_default_graph()\n",
    "        print(\"n_batches:\", n_batches)\n",
    "        for i in range(epochs):\n",
    "            if i % 10 == 0:\n",
    "                print(\"epoch:\", i)\n",
    "            np.random.shuffle(X)\n",
    "            for j in range(n_batches):\n",
    "                batch = X[j*batch_sz:(j+1)*batch_sz]\n",
    "                _, c, = self.sess.run((self.train_op, self.cost), feed_dict={self.X: batch})\n",
    "                c /= batch_sz # just debugging\n",
    "                costs.append(c)\n",
    "                #if j % 100 == 0:\n",
    "                    #print(\"iter: %d, cost: %.3f\" % (j, c)) \n",
    "        print('Mean cost in the last 10 epochs: ', np.mean(costs[-10:]))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.sess.run(self.X_hat, feed_dict={self.X: X})\n",
    "  \n",
    "    def get_data(self, df):    \n",
    "        np.random.seed(0)\n",
    "        data = df.as_matrix()\n",
    "        np.random.shuffle(data)\n",
    "        clf = StandardScaler()\n",
    "        X = clf.fit_transform(data[:,:])\n",
    "        Y = data[:,-1]\n",
    "        return clf, X, Y  \n",
    "\n",
    "def main(df):\n",
    "    #best = 6, 500, 750, 750, 500, 0.01\n",
    "    model = Autoencoder(6, 50, 75, 75, 50)\n",
    "    clf, X, Y = model.get_data(df) \n",
    "    model.fit(X)\n",
    " \n",
    "    count = 20000\n",
    "    res = []\n",
    "    while count>0 :\n",
    "        i = np.random.choice(len(X))\n",
    "        x = X[i]\n",
    "        im = model.predict([x])  \n",
    "        X = np.concatenate((X, im))\n",
    "        res.append(clf.inverse_transform(im[0])) \n",
    "        count -= 1 \n",
    "     \n",
    "    with open('new_data.pickle', 'wb') as handle:\n",
    "        pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have generated 10000 new records that resemble the original dataset. Now with this augmented dataset will create a Multi Layer perceptron model and see if there is any improvement in the predictions of the same  Testset as done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400  Loss:  212646.06\n",
      "390  Loss:  181022.0\n",
      "380  Loss:  187329.78\n",
      "370  Loss:  158733.36\n",
      "360  Loss:  113318.08\n",
      "350  Loss:  104555.19\n",
      "340  Loss:  108691.69\n",
      "330  Loss:  78295.17\n",
      "320  Loss:  78110.22\n",
      "310  Loss:  87130.234\n",
      "300  Loss:  79632.75\n",
      "290  Loss:  62102.805\n",
      "280  Loss:  46384.03\n",
      "270  Loss:  57186.516\n",
      "260  Loss:  42331.133\n",
      "250  Loss:  50443.83\n",
      "240  Loss:  36371.156\n",
      "230  Loss:  30936.605\n",
      "220  Loss:  38710.87\n",
      "210  Loss:  34308.49\n",
      "200  Loss:  25508.488\n",
      "190  Loss:  21488.484\n",
      "180  Loss:  21978.348\n",
      "170  Loss:  21360.012\n",
      "160  Loss:  19970.494\n",
      "150  Loss:  17761.242\n",
      "140  Loss:  16931.24\n",
      "130  Loss:  17693.107\n",
      "120  Loss:  13219.082\n",
      "110  Loss:  12302.67\n",
      "100  Loss:  12726.773\n",
      "90  Loss:  11815.604\n",
      "80  Loss:  12378.538\n",
      "70  Loss:  11392.732\n",
      "60  Loss:  12118.16\n",
      "50  Loss:  11869.362\n",
      "40  Loss:  10242.857\n",
      "30  Loss:  11162.77\n",
      "20  Loss:  11182.174\n",
      "10  Loss:  11845.91\n",
      "\n",
      "\n",
      "Predicted vs Actual Values \n",
      "\n",
      "257742712.75 299217189.78\n",
      "298949217.79 270822856.5\n",
      "200601155.38 261551483.26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import pickle, os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def augementedTrainModel(np_df, test_data): \n",
    "    np.random.seed(0)  \n",
    "    \n",
    "    clf = StandardScaler() \n",
    "    clf.fit(np_df)\n",
    "    np_df = clf.transform(np_df) \n",
    "    test_data = clf.transform(test_data) \n",
    "    \n",
    "    X_train, y_train = np_df[:, :-1], np_df[:, -1]\n",
    "    X_test, y_test = test_data[:, :-1], test_data[:, -1]  \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Graph().as_default():        \n",
    "        N, D = X_train.shape\n",
    "        \n",
    "        #Build 2 Hidden Layers with the following sizes (100, 100)\n",
    "        M = 100\n",
    "        H1 = H2 = 200\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, D))\n",
    "        y = tf.placeholder(tf.float32, shape=(None))\n",
    "       \n",
    "        # input -> hidden1\n",
    "        W = tf.Variable(tf.random_normal(shape=(D, M)) * np.sqrt(32.0 / (M+D)))\n",
    "        b = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "        \n",
    "        # hidden1 -> hidden2\n",
    "        W1 = tf.Variable(tf.random_normal(shape=(M, H1)) * np.sqrt(4.0 / (H1+M)))\n",
    "        b1 = tf.Variable(np.zeros(H1).astype(np.float32))\n",
    "        \n",
    "        # hidden2 -> hidden3, earlier had 3 hidden layers, M, H1, H2, now there are only 2 M, H1=H2\n",
    "        #W2 = tf.Variable(tf.random_normal(shape=(H1, H2)) * np.sqrt(32.0 / (H2+H1)))\n",
    "        #b2 = tf.Variable(np.zeros(H2).astype(np.float32))\n",
    "\n",
    "        # hidden3 -> output\n",
    "        V = tf.Variable(tf.random_normal(shape=(H2, 1)))\n",
    "        c = tf.Variable(np.zeros(1).astype(np.float32))\n",
    "        \n",
    "        #Adding dropouts\n",
    "        training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        dropout_rate = 0.95  # == 1 - keep_prob\n",
    "        #X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "        # construct the reconstruction\n",
    "        Z = tf.nn.tanh(tf.matmul(X , W) + b)\n",
    "        \n",
    "        Z_drop = tf.layers.dropout(Z, dropout_rate, training=training)\n",
    "        Zh1 = tf.nn.relu(tf.matmul(Z_drop, W1) + b1) \n",
    "        \n",
    "        Zh1_drop = tf.layers.dropout(Zh1, dropout_rate, training=training)\n",
    "        #Zh2 = tf.nn.tanh(tf.matmul(Zh1_drop, W2) + b2) \n",
    "        \n",
    "        #Zh2_drop = tf.layers.dropout(Zh2, dropout_rate, training=training)\n",
    "        logits = tf.matmul(Zh1_drop, V) + c\n",
    "        X_hat = (logits)\n",
    "\n",
    "        cost = tf.losses.mean_squared_error(logits, y) + tf.nn.l2_loss(V) \n",
    "        \n",
    "        # make the trainer, using RMS optimiser\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=0.00001).minimize(cost)\n",
    "\n",
    "        # set up session and initialise variables\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        n_epochs = 400\n",
    "        \n",
    "        #When deploying in GPU enabled machines, currently using GCP compute machine\n",
    "        gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "        config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init_op)\n",
    "            while n_epochs > 0:\n",
    "                batch_sz = 125\n",
    "                n_batches = len(X_train)//batch_sz\n",
    "                rnd_idx = np.random.permutation(len(X_train))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_sz): \n",
    "                    #batch_X = X_train[j*batch_sz:(j+1)*batch_sz]\n",
    "                    #batch_y = y_train[j*batch_sz:(j+1)*batch_sz]\n",
    "                    batch_X, batch_y = X_train[rnd_indices], y_train[rnd_indices]\n",
    "                    sess.run(train_op, feed_dict={X:batch_X, y:batch_y, training: True})  \n",
    "                if n_epochs%10 == 0:\n",
    "                    print(n_epochs, ' Loss: ', sess.run(cost, feed_dict={X:batch_X, y:batch_y}))\n",
    "                n_epochs = n_epochs - 1\n",
    "            res = sess.run(X_hat, feed_dict={X:X_test}) \n",
    "        print('\\n')\n",
    "        print('Predicted vs Actual Values', '\\n') \n",
    "        for i, test in enumerate(X_test):\n",
    "            pred = round(clf.inverse_transform(np.append(test, res[i]))[-1],2)\n",
    "            real = round(clf.inverse_transform(np.append(test, y_test[i]))[-1],2)\n",
    "            print(pred, real)\n",
    "             \n",
    "                                   \n",
    "if __name__ == '__main__':\n",
    "    with open('new_data.pickle', 'rb') as handle:\n",
    "        new_data = pickle.load(handle)\n",
    "    with open('test_records.pickle', 'rb') as handle:\n",
    "        test_data = pickle.load(handle)    \n",
    "    augementedTrainModel(np.array(new_data), np.array(test_data) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not much of an improvement there, I tried all combinations like, \n",
    "\n",
    "- with and without dropout\n",
    "\n",
    "- different activation functions \n",
    "\n",
    "- different weight initialisation values\n",
    "\n",
    "- different number of hidden layers and hidden layer sizes\n",
    "\n",
    "- different learning rates.\n",
    "\n",
    "Inspite of trying all combinations the results are not much better. Hence, Sticking to the original model with 17 records  for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Learning\n",
    "\n",
    "How to make a feedback learning model??\n",
    "\n",
    "One simple idea is inspired from Boosting algorithm in Decision Trees. Here, we give extra weight to the feedback record, by appending 2 records of the same feedback data to the dataset. Now, 2 copies of the feedback data are present in the dataset, so the model will learn this pattern more as compared to its previous version\n",
    "\n",
    "Other methods to feedback learning are not considered for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Again, printing First few records of our original dateset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Production_Count</th>\n",
       "      <th>Area_Count</th>\n",
       "      <th>Area_Sum</th>\n",
       "      <th>Kharif</th>\n",
       "      <th>Khariff</th>\n",
       "      <th>Production_Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3415</td>\n",
       "      <td>3415</td>\n",
       "      <td>75682249.0</td>\n",
       "      <td>26.64</td>\n",
       "      <td>329.360</td>\n",
       "      <td>1.382803e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4563</td>\n",
       "      <td>4708</td>\n",
       "      <td>83779058.0</td>\n",
       "      <td>26.79</td>\n",
       "      <td>368.052</td>\n",
       "      <td>1.610406e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5351</td>\n",
       "      <td>5373</td>\n",
       "      <td>78214722.0</td>\n",
       "      <td>26.63</td>\n",
       "      <td>334.555</td>\n",
       "      <td>1.107851e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5981</td>\n",
       "      <td>6025</td>\n",
       "      <td>86812995.0</td>\n",
       "      <td>26.61</td>\n",
       "      <td>289.456</td>\n",
       "      <td>1.142366e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5846</td>\n",
       "      <td>5873</td>\n",
       "      <td>87807709.4</td>\n",
       "      <td>26.90</td>\n",
       "      <td>289.739</td>\n",
       "      <td>1.277101e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Production_Count  Area_Count    Area_Sum  Kharif  Khariff  Production_Sum\n",
       "0              3415        3415  75682249.0   26.64  329.360    1.382803e+08\n",
       "1              4563        4708  83779058.0   26.79  368.052    1.610406e+08\n",
       "2              5351        5373  78214722.0   26.63  334.555    1.107851e+08\n",
       "3              5981        6025  86812995.0   26.61  289.456    1.142366e+08\n",
       "4              5846        5873  87807709.4   26.90  289.739    1.277101e+08"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Again, printing First few records of our original dateset')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback data for the year 1999 has total production 210785062.6 units instead of 110785062.6 units\n",
      "New dataset after appending 5 records of feedback data and shuffing it \n",
      "\n",
      "    Production_Count  Area_Count     Area_Sum  Kharif  Khariff  Production_Sum\n",
      "0             5350.0      5479.0  82601771.00   26.60  311.016    3.030534e+08\n",
      "1             3415.0      3415.0  75682249.00   26.64  329.360    1.382803e+08\n",
      "2             4339.0      4397.0  52000480.50   26.87  316.974    2.708229e+08\n",
      "3             5148.0      5303.0  82028612.17   26.72  336.148    2.615515e+08\n",
      "4             5981.0      6025.0  86812995.00   26.61  289.456    1.142366e+08\n",
      "5             5335.0      5510.0  82910545.47   26.60  352.631    2.910593e+08\n",
      "6             5412.0      5552.0  80700959.42   26.44  298.543    2.399453e+08\n",
      "7             5351.0      5373.0  78214722.00   26.63  334.555    2.107851e+08\n",
      "8             6145.0      6161.0  88706585.43   26.61  331.896    1.382855e+08\n",
      "9             4757.0      4906.0  72563659.00   27.00  294.109    2.952903e+08\n",
      "10            5221.0      5286.0  66443853.00   26.80  345.948    2.992172e+08\n",
      "11            5223.0      5366.0  75118880.56   26.75  332.394    2.781617e+08\n",
      "12            5589.0      5723.0  89264102.81   26.76  354.293    3.653053e+08\n",
      "13            5846.0      5873.0  87807709.40   26.90  289.739    1.277101e+08\n",
      "14            5351.0      5373.0  78214722.00   26.63  334.555    2.107851e+08\n",
      "15            5805.0      5865.0  80031223.00   26.90  250.052    9.665179e+07\n",
      "16            5347.0      5472.0  70839304.00   26.64  367.336    2.525192e+08\n",
      "17            5433.0      5514.0  69277092.00   26.70  325.184    2.859547e+08\n",
      "18            4563.0      4708.0  83779058.00   26.79  368.052    1.610406e+08\n",
      "19            5351.0      5373.0  78214722.00   26.63  334.555    2.107851e+08\n"
     ]
    }
   ],
   "source": [
    "print('Feedback data for the year 1999 has total production 210785062.6 units instead of 110785062.6 units')\n",
    "\n",
    "#Suppose the year 1999 record is bad... and need to be change Production_Sum from 110785062.6 to 210785062.6 \n",
    "df['Production_Sum'][2]=210785062.6 \n",
    "for i in range(2):\n",
    "    df = df.append(df.iloc[2])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Shuffle the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print('New dataset after appending 5 records of feedback data and shuffing it', '\\n')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivanand/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  Loss:  259.93698\n",
      "960  Loss:  241.49875\n",
      "920  Loss:  227.77632\n",
      "880  Loss:  215.77844\n",
      "840  Loss:  205.35513\n",
      "800  Loss:  196.35902\n",
      "760  Loss:  188.66354\n",
      "720  Loss:  182.1679\n",
      "680  Loss:  176.78207\n",
      "640  Loss:  172.4085\n",
      "600  Loss:  168.91315\n",
      "560  Loss:  166.15695\n",
      "520  Loss:  164.01193\n",
      "480  Loss:  162.38135\n",
      "440  Loss:  161.19255\n",
      "400  Loss:  160.37042\n",
      "360  Loss:  159.82956\n",
      "320  Loss:  159.45758\n",
      "280  Loss:  159.15453\n",
      "240  Loss:  158.88919\n",
      "200  Loss:  158.6551\n",
      "160  Loss:  158.44862\n",
      "120  Loss:  158.26712\n",
      "80  Loss:  158.10803\n",
      "40  Loss:  157.9691\n",
      "\n",
      "\n",
      "Predicted vs Actual Values \n",
      "\n",
      "238004397.51 285954655.53\n",
      "232669795.76 161040570.0\n",
      "224897236.23 210785062.6\n"
     ]
    }
   ],
   "source": [
    "#Feedback Learning\n",
    "np_df = df.as_matrix() \n",
    "clf = StandardScaler() \n",
    "clf.fit(np_df)\n",
    "np_df = clf.transform(np_df)\n",
    "X = np_df[:,:-1]\n",
    "y = np_df[:, -1]\n",
    "#Let's have the last 3 records as Test Records , feedback data is a part of Test records\n",
    "X_train, X_test = X[:-3, :], X[(-3,-2,-1), :] \n",
    "y_train, y_test = y[:-3], y[[-3,-2,-1]] \n",
    "trainModel(X_train, X_test, y_train, y_test, clf, is_feedback=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the last record, which is the feedback record, the model has it learnt fairly decently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "-> Combined temperature data, Rainfall data and Crop production data. There are only 17 records, one record per year from 1997 to 2014\n",
    "\n",
    "-> In the first attempt, Built a MLP neural network model using Tensorflow Libraries with these 17 records. The performance on test set is OK.\n",
    "\n",
    "-> In the second attempt, Build a Autoencoder model to generate additional data (10000) out of these 17 records and then built another MLP neural network model using this augmented dataset(10017 records). Tried a different number of hyperparameter tuning like with and without dropouts, different learning rates, different activation functions and different initial weights. But the results were not significantly better. Hence stuck to the original model with 17 records\n",
    "\n",
    "-> Feedback Learning: Inspired from Boosting technique. Generally feedback data is very small compared to the training dataset, hence made multiple copies of feedback data, in this case 2, and augmented the training dataset with multiple copies feedback data. Re-trained the model ( infact, restored the earlier model weights and then retrained ). This is a simple feedback learning mechanism. Since we are training on top of earlier trained model, the previous correct patterns should also have been preseved along with learning new pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
